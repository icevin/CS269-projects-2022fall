---
layout: post
comments: true
title: Room rearrangement and indoor environment exploration
author: Dylon Tjanaka, Kevin Tang, and Daniel Smith (Team 20)
date: 2022-10-19
---

> We investigate the problem of room rearrangement, where an agent explores a room, then attempts to restore the objects in a room to their original state.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
In this project, we will investigate the problem of room rearrangement. In this task, an agent first explores a room and records the configurations of objects, using visual information available to it including RGB images and depth maps. After changing the poses/states of some objects in the room without the agent present, the agent must then restore the room to its original state.

In particular, we will study the improvement that applying various techniques brings, including discussing the incredible performance improvements that utilizing contrastive language image pretraining (CLIP) embeddings bring. This fundamentally simple, yet extremely effective technique has found to be effective in varied applications, from the image recognition tasks where it outperforms supervised models, to other natural language tasks like visual question answering. It has even been applied in image generation tasks, including StyleGAN image manipulation and prominently, the recent explosion in image generation models like DALLE-2 and Stable Diffusion [Khandewal].

We will also study the design of successful agents. With as complex of a task as room rearrangement, succesful agents may combine several reinforcement learning techniques to build submodules to solve subtasks that ultimately build to accomplish the full task.

## Environment
Our explorations will be based on the [AI2-THOR](https://ai2thor.allenai.org/) open source interactive environment, which allows for AI agents to interact with a realistic 3D environment, with many built in tools for checkpointing and validation. Agents have 82 actions available to them, including moving, rotating, and/or looking in any direction, and manipulating objects in the environment. We will use the RoomR dataset compiled by [Weihs et al., 2021]. 

## Task
Given some environment consisting of a room and some number of objects, the agent has to complete two phases. The first phase is the "walkthrough" phase, where our agent explores the room, and observes the objects in their intended goal state. 
The second phase is the "unshuffling" or restoration phase, where a random number of objects in the room are changed, between one and five objects. The goal is to not only identify which objects have changed, but to manipulate the objects to restore them to their initial state in the walkthrough phase. Objects may be changed by moving its position, rotating or otherwise changing its orientation, or modifying some aspect of the object, including opening/closing the object.

After some initial exploration, we decided to evaluate the simpler variation of the problem. Instead of performing two separate, distinct stages, one walkthrough and one restoration, the "1-phase" challenge merges both the phases into one. In this simplification, the agent has access to one RGB image from the goal state as well as one image from the shuffle state simultaneously, and need not make two trips throughout the environment to first gather data, and then perform the manipulations. We decided to make this simplification to lower model complexity.

<!-- image of phase1/phase2 -->

Agents receieve two types of input from the environment: a 224×224×3 RGB image, and a 1x6 AgentPose state vector that details exactly where the agent is current located.

## Model Overview

The various agents tasked with solving this problem have a similar fundamental structure. Essentially, the agent must know how to encode the 224x224x3 RGB images into a "perception" unit, and reason over time with a "planning" unit.

### Perception
The perception component of the agent must take in the 224x224x3 RGB image and discern important information like the objects in the current view, as well as depth and positional information.
#### ResNet
One of the leading CNN backbonse, ResNet has proven to be adept at encoding visual information. In our baseline models, we evaluate agents that utilize ResNet as a visual encoder.

#### CLIP

At its heart, the ideas behind contrastive language image pretraining are fairly simple. CLIP encoders are trained on image-caption pairs, and learns to distinguish between image-caption pairs that correlate and image-caption pairs that contrast. Scaled up to a training dataset of 400M image-caption pairs, the result is a visual encoder that is incredibly adept. For example, when evaluated on the ImageNet benchmark, in zero-shot setting (no additional training), a CLIP visual encoder outperformed a fully supervised ResNet-50 model.
<!-- clip paper diagram -->

![CLIP Architecture](https://ucla-rlcourse.github.io/CS269-projects-2022fall/assets/images/team20/clip_diagram.png)

In this application, CLIP is similarly pitted against ResNet-50 above as a visual encoder.

### Planning

With both agent position information and encoded visual information, the agent must be able to reason over time. However, since the problem cannot be described by a Markov decision process, we must also utilize a Long Short-Term Memory.

The agents under comparison are all actor-critic agents. Given observations $\omega_t$, a history $h_{t-1}$, our actor-critic agent produces a policy $\pi_{\theta}(\omega_t | h_{t-1})$ and a value $v_{\theta}(\omega_t | h_{t-1})$, where $\theta$ is a generalized set of parameters. 

While the agents under comparsion all use some combination of techniques like PPO, Imitation Learning, and DAgger, among others, the primary agents we will be studying today use imitation learning, with expert actions provided by another reinforcement learning agent that acts with perfect information of the objects in the scene. This agent simply picks the closest, incorrectly placed object, navigates to it, and corrects its location.

## Evaluation
We will score our different models on a few key factors. One overall metric is known as Success, or simply whether or not all objects are in their goal states at the end of the agent's actions. However, this metric is quite unforgiving. If objects are in the correct position, but not in the correct orientation, that object is still counted as incorrect. Therefore, we also include a second metric, known as Misplaced. This metric counts the percentage of objects that are simply misplaced, giving us more information about how the model performs.

## Experiments
We will compare ...

TODO

## Results



## References
[1] Weihs, L., Deitke, M., Kembhavi, A., & Mottaghi, R. (2021). Visual Room Rearrangement (Version 1). arXiv https://doi.org/10.48550/ARXIV.2103.16544 

[2] Dhruv Batra, Angel Xuan Chang, S. Chernova, Andrew J. Davison, Jun Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, and Hao Su. Rearrangement: A challenge for embodied ai. arXiv, 2020.

[3] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, Aniruddha Kembhavi. Simple but Effective: CLIP Embeddings for Embodied AI. arXiv, 2022.